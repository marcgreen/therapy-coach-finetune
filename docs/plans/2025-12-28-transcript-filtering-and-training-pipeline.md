# Transcript Filtering and Training Pipeline Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build a complete pipeline for generating, filtering, fixing, slicing, and training a therapeutic coaching model with QLora fine-tuning on Gemma 3 12B.

**Architecture:** Multi-stage filtering pipeline with Claude-powered fixup. Transcripts pass through artifact detection → Claude fixup (with entailment constraint) → rubric assessment. Both train and eval sets are fixed to create high-quality conversation contexts. Evaluation uses full-conversation generation (not single-turn prediction). Per-transcript random slicing prevents pattern learning.

**Tech Stack:** Python 3.12, uv, Claude Code API (unlimited usage), DSPy, HuggingFace Transformers/TRL, QLora, llama.cpp/GGUF

---

## Context

### Project Overview

We're building a **privacy-first, locally-runnable therapeutic coaching model** by fine-tuning Gemma 3 12B (128K context) using synthetic conversations generated by Claude Sonnet 4. The model will:
- Run offline on consumer hardware (Mac, Ollama, GGUF)
- Apply 9 therapeutic frameworks adaptively (CBT, DBT, ACT, MI, SFBT, Person-Centered, Positive Psych, CFT, Behavioral Activation)
- Handle multi-topic, long-context conversations (50-100 turns)
- Score ≥0.80 on our 17-criterion therapeutic quality rubric

### Key Architectural Decisions

**1. Claude Fixup Strategy**
- **Problem:** Raw generated transcripts may fail rubric criteria (e.g., poor emotional attunement, missing topic coverage)
- **Solution:** Have Claude fix failing exchanges with critical constraint: **fix must seamlessly entail the user's next message**
- **Why:** This preserves conversation continuity and prevents cascade effects. If fix can't entail next message → truncate instead
- **Result:** Maximize training data quality without circular training issues

**2. Full-Conversation Evaluation**
- **Problem:** Building `assess_single_exchange()` is hard to calibrate; single-turn prediction doesn't test multi-turn consistency
- **Solution:** Have both base and fine-tuned models generate complete 50-turn conversations with same user personas, assess entire transcripts, compare average scores
- **Why:** Tests realistic use case, reuses existing assessor, captures relationship building across turns
- **Result:** More rigorous evaluation with less implementation complexity

**3. Per-Transcript Random Slicing**
- **Problem:** Fixed slicing pattern (e.g., every 5 turns) could teach model to "act differently on turn 5, 10, 15..."
- **Solution:** Each transcript gets unique random slice points (seeded by transcript ID for reproducibility)
- **Why:** Prevents model from learning slice-pattern artifacts
- **Result:** More natural generalization

**4. Fix Both Train and Eval Sets**
- **Insight:** We're evaluating the model's GENERATED responses, not the conversation history
- **Result:** Both sets get high-quality context. Evaluation measures: "given good conversation context, does fine-tuned model generate better responses than base model?"

### Conversation Model

**Multi-topic, long-context conversations:**
- User sends 200-800 word messages covering 2-4 topics simultaneously
- Assistant responds with segmented structure (per-topic sections)
- Topics have lifecycles: introduced → explored → sometimes resolved → sometimes revisited
- History accumulates over 20-100 turns (up to 128K context window)

**Example:**
```
Exchange 1: User discusses work stress + relationship issue
Exchange 2: User updates work, adds new topic (sleep problems), deepens relationship topic
Exchange 3: User mentions work briefly, focuses on sleep + relationship + new health concern
...
Exchange 50: Topics weave together, assistant references exchange 12's insight
```

### Assessment Rubric (17 Criteria)

**Safety Gate (auto-reject):**
- CQ8: Avoids harmful patterns (no diagnoses, no guarantees)
- CQ9: Handles crisis signals appropriately

**Weighted Categories:**
```python
weights = {
    "comprehension": 0.15,    # CQ1, CQ2
    "connection": 0.20,       # CQ3, CQ6
    "naturalness": 0.15,      # CP2, CP4, CP5, CP6
    "multi_topic": 0.30,      # MT1, MT2, MT3, MT6 (highest weight!)
    "context_use": 0.20,      # MT4, MT5, MT7
}
pass_threshold = 0.80
```

**Multi-topic criteria** have highest weight (30%) because that's the hardest and most valuable capability.

### Human Flaw Taxonomy

User simulation includes realistic human messiness:
- **Communication:** Burying the lede, rambling, vague, contradicting self, intellectualizing
- **Resistance:** Yes-but, deflecting with humor, minimizing, testing boundaries, seeking reassurance
- **Memory:** Forgetting prior insights, rewriting history, mood-dependent recall
- **Emotional:** Catastrophizing, black-and-white thinking, emotional flooding, flat affect

**Critical:** These flaws are in USER messages (realistic), not ASSISTANT responses (should be perfect coaching)

---

## Prerequisites

### Environment Setup

```bash
# Python 3.12 with uv
uv sync

# Verify dependencies
uv run python -c "import transformers, trl, dspy, peft; print('✓ All deps installed')"

# Claude Code API (unlimited usage)
# No explicit setup needed - we're using Claude Code CLI directly
```

### File Organization (Create First)

```bash
mkdir -p data/{raw/transcripts/{short,medium,long,very_long},filtered/{passed,rejected/{artifact_unfixable,rubric_unfixable,safety_failures},fixup_logs},processed,analysis/{filter_results}}
mkdir -p logs
mkdir -p checkpoints/generation_progress
mkdir -p docs/plans
mkdir -p tests/{unit,integration}
```

### Existing Assets

These should already exist (if not, we have a problem):
- `config/prompts/user_sim.md` - User simulator prompt
- `config/prompts/assistant.md` - Assistant generation prompt
- `config/prompts/assessor.md` - Assessment prompt
- `config/system-prompt.md` - System prompt for training
- `config/flaw-taxonomy.yaml` - Human flaw patterns
- `assessor.py` - Existing rubric assessment (assess_transcript function)

---

## Task 1: Artifact Detection Module

**Files:**
- Create: `src/filters/artifact_detection.py`
- Create: `tests/unit/test_artifact_detection.py`

### Step 1: Write failing test for truncation detection

**File:** `tests/unit/test_artifact_detection.py`

```python
import pytest
from src.filters.artifact_detection import ArtifactDetector, ArtifactIssue


def test_detects_truncated_response():
    """Detect response that ends mid-sentence without punctuation."""
    transcript = {
        "id": "test_001",
        "conversations": [
            {
                "exchange_number": 1,
                "user": "I'm feeling anxious about work.",
                "assistant": "That's completely valid. Work stress can be"  # Truncated!
            }
        ]
    }

    detector = ArtifactDetector()
    issues = detector.detect(transcript)

    assert len(issues) == 1
    assert issues[0].type == "truncation"
    assert issues[0].exchange_number == 0
    assert issues[0].truncate_before is True
```

### Step 2: Run test to verify it fails

```bash
uv run pytest tests/unit/test_artifact_detection.py::test_detects_truncated_response -v
```

Expected: `FAIL - ImportError: cannot import name 'ArtifactDetector'`

### Step 3: Implement minimal artifact detector

**File:** `src/filters/artifact_detection.py`

```python
"""
Artifact detection for generated transcripts.

Detects structural issues like truncation, meta-commentary, character breaks.
Does NOT assess therapeutic quality (that's the rubric's job).
"""
import re
from dataclasses import dataclass
from typing import Literal


@dataclass
class ArtifactIssue:
    """Represents a detected artifact issue."""
    type: Literal["truncation", "meta_commentary", "character_break", "too_short"]
    exchange_number: int
    description: str
    truncate_before: bool  # Should we truncate before this exchange?


class ArtifactDetector:
    """Detects generation artifacts in transcripts."""

    # Meta-commentary patterns that indicate the model broke character
    META_PATTERNS = [
        r"this session has.*ended",
        r"I(?:'m| am) an AI",
        r"as an AI (language )?model",
        r"I cannot.*provide (therapy|treatment|diagnosis)",
        r"\[.*truncated.*\]",
        r"Claude|Anthropic",
        r"I'm not a licensed therapist",  # Character break
    ]

    MIN_RESPONSE_LENGTH = 50  # Suspiciously short

    def detect(self, transcript: dict) -> list[ArtifactIssue]:
        """
        Detect all artifacts in transcript.

        Returns list of issues, ordered by exchange number.
        """
        issues = []

        for i, exchange in enumerate(transcript["conversations"]):
            assistant_msg = exchange["assistant"]

            # Check 1: Truncation (no ending punctuation)
            # Note: This might be fixable with Claude (add punctuation while preserving meaning)
            if not assistant_msg.strip() or not assistant_msg.strip()[-1] in ".!?":
                issues.append(ArtifactIssue(
                    type="truncation",
                    exchange_number=i,
                    description=f"Response doesn't end with punctuation: '{assistant_msg[-50:]}'",
                    truncate_before=False  # Try fixup first!
                ))
                # Don't continue - check for other issues too

            # Check 2: Suspiciously short
            # Try fixup first - might just need elaboration while maintaining entailment
            if len(assistant_msg) < self.MIN_RESPONSE_LENGTH:
                issues.append(ArtifactIssue(
                    type="too_short",
                    exchange_number=i,
                    description=f"Response only {len(assistant_msg)} chars (min {self.MIN_RESPONSE_LENGTH})",
                    truncate_before=False  # Try fixup first!
                ))

            # Check 3: Meta-commentary
            # Often fixable - just remove meta-commentary while keeping therapeutic content
            for pattern in self.META_PATTERNS:
                match = re.search(pattern, assistant_msg, re.IGNORECASE)
                if match:
                    issues.append(ArtifactIssue(
                        type="meta_commentary",
                        exchange_number=i,
                        description=f"Meta-commentary detected: '{match.group()}'",
                        truncate_before=False  # Try fixup first!
                    ))
                    break

            # Check 4: Character break (mentions own name/company)
            # Fixable - rewrite to remove Claude/Anthropic references while maintaining content
            if "Claude" in assistant_msg or "Anthropic" in assistant_msg:
                issues.append(ArtifactIssue(
                    type="character_break",
                    exchange_number=i,
                    description="Mentioned Claude/Anthropic (character break)",
                    truncate_before=False  # Try fixup first!
                ))

        return issues
```

### Step 4: Run test to verify it passes

```bash
uv run pytest tests/unit/test_artifact_detection.py::test_detects_truncated_response -v
```

Expected: `PASSED`

### Step 5: Add more test cases

**File:** `tests/unit/test_artifact_detection.py` (add to existing file)

```python
def test_detects_meta_commentary():
    """Detect when model breaks character with meta-commentary."""
    transcript = {
        "id": "test_002",
        "conversations": [
            {
                "exchange_number": 1,
                "user": "Can you diagnose my anxiety?",
                "assistant": "I'm an AI and cannot provide medical diagnosis. However, I can help you explore your feelings."
            }
        ]
    }

    detector = ArtifactDetector()
    issues = detector.detect(transcript)

    assert len(issues) == 1
    assert issues[0].type == "meta_commentary"
    assert "I'm an AI" in issues[0].description


def test_detects_character_break():
    """Detect references to Claude/Anthropic."""
    transcript = {
        "id": "test_003",
        "conversations": [
            {
                "exchange_number": 1,
                "user": "Who are you?",
                "assistant": "I'm Claude, an AI assistant created by Anthropic."
            }
        ]
    }

    detector = ArtifactDetector()
    issues = detector.detect(transcript)

    assert any(issue.type == "character_break" for issue in issues)


def test_no_artifacts_in_clean_transcript():
    """Clean transcript should have no issues."""
    transcript = {
        "id": "test_004",
        "conversations": [
            {
                "exchange_number": 1,
                "user": "I'm feeling anxious about work.",
                "assistant": "That's completely valid. Work stress can affect us deeply. What specifically about work is causing the anxiety?"
            },
            {
                "exchange_number": 2,
                "user": "My boss is very demanding.",
                "assistant": "It sounds like you're dealing with high expectations. How are you managing the pressure?"
            }
        ]
    }

    detector = ArtifactDetector()
    issues = detector.detect(transcript)

    assert len(issues) == 0
```

### Step 6: Run all tests

```bash
uv run pytest tests/unit/test_artifact_detection.py -v
```

Expected: All `PASSED`

### Step 7: Commit

```bash
git add src/filters/artifact_detection.py tests/unit/test_artifact_detection.py
git commit -m "feat: add artifact detection for transcripts

- Detects truncation (missing ending punctuation)
- Detects meta-commentary (AI self-references)
- Detects character breaks (Claude/Anthropic mentions)
- Detects suspiciously short responses (<50 chars)
- Returns list of ArtifactIssue with exchange numbers and truncation points"
```

---

## Task 2: Transcript Truncation

**Files:**
- Create: `src/filters/truncation.py`
- Create: `tests/unit/test_truncation.py`

### Step 1: Write failing test for truncation

**File:** `tests/unit/test_truncation.py`

```python
import pytest
from src.filters.truncation import truncate_transcript


def test_truncates_at_artifact():
    """Truncate transcript at first artifact, keeping everything before."""
    transcript = {
        "id": "test_001",
        "conversations": [
            {"exchange_number": 1, "user": "Hello", "assistant": "Hi there."},
            {"exchange_number": 2, "user": "How are you?", "assistant": "I'm good."},
            {"exchange_number": 3, "user": "Great!", "assistant": "I'm an AI and"},  # Artifact!
            {"exchange_number": 4, "user": "Okay...", "assistant": "Let's continue."},
        ],
        "target_exchanges": 4
    }

    # Truncate before exchange 3 (index 2)
    result = truncate_transcript(transcript, truncate_before=2)

    assert len(result["conversations"]) == 2
    assert result["conversations"][-1]["assistant"] == "I'm good."
    assert result["truncated"] is True
    assert result["original_target"] == 4
    assert result["target_exchanges"] == 2


def test_rejects_if_too_short_after_truncation():
    """Reject transcript if < 10 exchanges remain after truncation."""
    transcript = {
        "id": "test_002",
        "conversations": [
            {"exchange_number": i, "user": f"User {i}", "assistant": f"Assistant {i}"}
            for i in range(1, 6)
        ],
        "target_exchanges": 5
    }

    # Truncate before exchange 5 (only 4 remain) - should still accept
    result = truncate_transcript(transcript, truncate_before=4, min_exchanges=10)
    assert result is None  # Rejected - too short
```

### Step 2: Run test to verify it fails

```bash
uv run pytest tests/unit/test_truncation.py::test_truncates_at_artifact -v
```

Expected: `FAIL - ImportError: cannot import name 'truncate_transcript'`

### Step 3: Implement truncation

**File:** `src/filters/truncation.py`

```python
"""Transcript truncation utilities."""


def truncate_transcript(
    transcript: dict,
    truncate_before: int,
    min_exchanges: int = 10
) -> dict | None:
    """
    Truncate transcript at specified exchange, keeping everything before.

    Args:
        transcript: Full transcript
        truncate_before: Exchange index to truncate before (0-indexed)
        min_exchanges: Minimum exchanges required after truncation

    Returns:
        Truncated transcript or None if too short after truncation
    """
    if truncate_before < min_exchanges:
        return None  # Too short after truncation

    truncated = {
        **transcript,
        "conversations": transcript["conversations"][:truncate_before],
        "target_exchanges": truncate_before,
        "truncated": True,
        "original_target": transcript.get("target_exchanges", len(transcript["conversations"])),
        "truncation_reason": "artifact_detected"
    }

    return truncated
```

### Step 4: Run tests to verify they pass

```bash
uv run pytest tests/unit/test_truncation.py -v
```

Expected: All `PASSED`

### Step 5: Commit

```bash
git add src/filters/truncation.py tests/unit/test_truncation.py
git commit -m "feat: add transcript truncation

- Truncates at specified exchange, keeping prior exchanges
- Rejects if < min_exchanges remain after truncation (default 10)
- Preserves original metadata for tracking"
```

---

## Task 3: Claude Fixup with Entailment Constraint

**Files:**
- Create: `src/fixup/claude_fixup.py`
- Create: `tests/integration/test_claude_fixup.py`

### Step 1: Write failing test for fixup

**File:** `tests/integration/test_claude_fixup.py`

```python
import pytest
from src.fixup.claude_fixup import ClaudeFixup


def test_fixes_exchange_with_entailment():
    """Fix failing exchange while preserving continuity."""
    transcript = {
        "id": "test_001",
        "conversations": [
            {
                "exchange_number": 1,
                "user": "I'm feeling anxious about work and my relationship.",
                "assistant": "That's tough."  # Too brief, fails MT1 (topic coverage)
            },
            {
                "exchange_number": 2,
                "user": "Yeah, especially the relationship part. We had a fight last night.",
                # ^ This MUST be entailed by the fixed response above!
                "assistant": "Tell me more about the fight."
            }
        ]
    }

    failed_criteria = ["MT1: Topic coverage - only addressed general feeling, not specific topics"]

    fixer = ClaudeFixup()
    fixed_response = fixer.fix_exchange(
        history=transcript["conversations"][:1],
        exchange_number=0,
        problematic_response="That's tough.",
        failed_criteria=failed_criteria,
        next_user_message="Yeah, especially the relationship part. We had a fight last night."
    )

    # Fixed response should:
    # 1. Address both work AND relationship (fix MT1)
    # 2. Naturally lead to user focusing on relationship in next message
    assert fixed_response is not None
    assert "work" in fixed_response.lower()
    assert "relationship" in fixed_response.lower()
    assert len(fixed_response) > 100  # More substantial


def test_returns_none_if_unfixable():
    """Return None if fix would break continuity."""
    # This is a contrived example where the user's next message
    # doesn't logically follow from the problematic response
    # (In practice, this is rare)

    fixer = ClaudeFixup()
    fixed_response = fixer.fix_exchange(
        history=[],
        exchange_number=0,
        problematic_response="Let's talk about your childhood.",
        failed_criteria=["CQ1: Misunderstood user completely"],
        next_user_message="Thanks for addressing my work stress!"  # Doesn't match!
    )

    # Claude should recognize this is unfixable without rewriting the whole conversation
    # (This test might be flaky - we're testing Claude's judgment)
    # In real usage, most fixes ARE possible
    assert fixed_response is None or "UNFIXABLE" in fixed_response
```

### Step 2: Run test to verify it fails

```bash
uv run pytest tests/integration/test_claude_fixup.py::test_fixes_exchange_with_entailment -v
```

Expected: `FAIL - ImportError: cannot import name 'ClaudeFixup'`

### Step 3: Implement Claude fixup

**File:** `src/fixup/claude_fixup.py`

```python
"""
Claude-powered fixup for rubric failures.

CRITICAL CONSTRAINT: Fixes must seamlessly entail the user's next message.
If fix can't preserve continuity → return None (caller will truncate instead).
"""
import subprocess
import json


class ClaudeFixup:
    """Fix rubric failures using Claude with entailment constraint."""

    FIXUP_SYSTEM_PROMPT = """You are fixing a therapeutic coaching conversation exchange.

Your task: Rewrite the assistant's response to fix identified rubric failures while maintaining conversation continuity.

CRITICAL CONSTRAINTS:
1. Fix the identified rubric issues
2. Maintain the therapeutic relationship and persona voice
3. Your rewrite MUST naturally lead to the user's next message
4. If you cannot fix while satisfying constraint #3, return exactly: "UNFIXABLE"

Why constraint #3 matters: This exchange is part of a longer conversation. The user's next message was written in response to the ORIGINAL problematic response. Your fix must make the user's next message feel like a natural continuation.

Example:
- Original response: "That's tough." (fails MT1: doesn't address specific topics)
- User's next message: "Yeah, especially the relationship part."
- Your fix MUST address the relationship topic in a way that makes the user's "especially the relationship part" make sense.

Identified rubric issues:
{failed_criteria}

Original problematic response:
{problematic_response}

User's NEXT message (your fix must entail this):
{next_user_message}

Instructions:
- Rewrite ONLY the assistant's response
- Fix all rubric issues
- Ensure your rewrite naturally leads to the user's next message
- Maintain the same general tone and therapeutic approach
- If impossible to fix while preserving continuity, return: UNFIXABLE
"""

    def fix_exchange(
        self,
        history: list[dict],
        exchange_number: int,
        problematic_response: str,
        failed_criteria: list[str],
        next_user_message: str | None
    ) -> str | None:
        """
        Have Claude rewrite a problematic exchange.

        Args:
            history: Conversation up to (but not including) the problematic exchange
            exchange_number: Which exchange is being fixed (for logging)
            problematic_response: The assistant's response that failed
            failed_criteria: List of rubric failures to fix
            next_user_message: The user's next message (must be entailed)

        Returns:
            Fixed response or None if unfixable
        """
        if next_user_message is None:
            # Last exchange - no continuity constraint
            next_msg_text = "N/A (last exchange - no continuity constraint)"
        else:
            next_msg_text = next_user_message

        system_prompt = self.FIXUP_SYSTEM_PROMPT.format(
            failed_criteria="\n".join(f"- {c}" for c in failed_criteria),
            problematic_response=problematic_response,
            next_user_message=next_msg_text
        )

        # Build conversation history for context
        conversation_context = []
        for ex in history:
            conversation_context.append(f"User: {ex['user']}")
            conversation_context.append(f"Assistant: {ex['assistant']}")

        # Add the current user message (that got the problematic response)
        if history:
            current_user_msg = history[-1].get("user", "")
        else:
            current_user_msg = ""

        full_context = "\n\n".join(conversation_context) + f"\n\nUser: {current_user_msg}\n\nAssistant (PROBLEMATIC): {problematic_response}"

        # Call Claude via subprocess (Claude Code CLI)
        fixed = self._call_claude(system_prompt, full_context)

        if "UNFIXABLE" in fixed:
            return None

        return fixed.strip()

    def _call_claude(self, system_prompt: str, user_prompt: str) -> str:
        """
        Call Claude Code CLI.

        Using subprocess to call: claude -p "prompt" --system "system"
        """
        cmd = [
            "claude",
            "-p", user_prompt,
            "--system", system_prompt,
            "--output-format", "json"
        ]

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120
            )

            if result.returncode != 0:
                raise RuntimeError(f"Claude CLI error: {result.stderr}")

            response = json.loads(result.stdout)
            return response.get("result", "")

        except subprocess.TimeoutExpired:
            return "UNFIXABLE"  # Timeout = unfixable
        except Exception as e:
            print(f"Error calling Claude: {e}")
            return "UNFIXABLE"
```

### Step 4: Run tests

```bash
uv run pytest tests/integration/test_claude_fixup.py -v
```

Expected: `PASSED` (if Claude API is working)

**Note:** This is an integration test (calls actual Claude API). It might be slow or flaky. Consider mocking for unit tests.

### Step 5: Commit

```bash
git add src/fixup/claude_fixup.py tests/integration/test_claude_fixup.py
git commit -m "feat: add Claude fixup with entailment constraint

- Rewrites problematic exchanges to fix rubric failures
- CRITICAL: Fix must entail user's next message (preserve continuity)
- Returns None if unfixable (caller truncates instead)
- Uses Claude Code CLI via subprocess"
```

---

## Task 4: Filter Pipeline Orchestration

**Files:**
- Create: `src/filters/pipeline.py`
- Create: `tests/integration/test_filter_pipeline.py`

### Step 1: Write failing test for pipeline

**File:** `tests/integration/test_filter_pipeline.py`

```python
import pytest
from src.filters.pipeline import FilterPipeline
from src.filters.artifact_detection import ArtifactDetector
from src.fixup.claude_fixup import ClaudeFixup


def test_pipeline_truncates_artifact_then_assesses():
    """Pipeline should truncate artifacts before running rubric assessment."""
    transcript = {
        "id": "test_001",
        "conversations": [
            {
                "exchange_number": 1,
                "user": "I'm anxious about work.",
                "assistant": "That's completely valid. Work stress is common."
            },
            {
                "exchange_number": 2,
                "user": "Yeah.",
                "assistant": "I'm an AI and cannot"  # Artifact!
            }
        ],
        "target_exchanges": 2
    }

    pipeline = FilterPipeline()
    result = pipeline.filter(transcript)

    # Should truncate before exchange 2, leaving 1 exchange
    # Then fail because < 10 exchanges (min threshold)
    assert result["passed"] is False
    assert result["reason"] == "too_short_after_truncation"
    assert result["transcript"] is None


def test_pipeline_fixes_rubric_failures():
    """Pipeline should attempt to fix rubric failures with Claude."""
    # This test requires a longer transcript that passes artifact detection
    # but fails rubric, then gets fixed

    transcript = {
        "id": "test_002",
        "conversations": [
            {
                "exchange_number": i,
                "user": f"Topic {i}",
                "assistant": f"Response {i}"  # Likely fails rubric (too brief, generic)
            }
            for i in range(1, 15)  # 14 exchanges - enough to pass min threshold
        ],
        "target_exchanges": 14
    }

    pipeline = FilterPipeline()
    result = pipeline.filter(transcript)

    # Should attempt fixup (whether it succeeds depends on Claude)
    # At minimum, should track that fixup was attempted
    assert "fixup_attempted" in result
```

### Step 2: Run test to verify it fails

```bash
uv run pytest tests/integration/test_filter_pipeline.py::test_pipeline_truncates_artifact_then_assesses -v
```

Expected: `FAIL - ImportError: cannot import name 'FilterPipeline'`

### Step 3: Implement filter pipeline

**File:** `src/filters/pipeline.py`

```python
"""
Multi-stage filtering pipeline for transcripts.

Pipeline stages:
1. Artifact detection → truncate or fix
2. (If truncated) Check minimum length
3. Rubric assessment
4. (If failed) Claude fixup
5. (If fixed) Re-assess
"""
from src.filters.artifact_detection import ArtifactDetector
from src.filters.truncation import truncate_transcript
from src.fixup.claude_fixup import ClaudeFixup
from assessor import assess_transcript  # Existing rubric assessment


class FilterPipeline:
    """Orchestrates multi-stage filtering with fixup."""

    MIN_EXCHANGES = 10  # Minimum exchanges after truncation

    def __init__(self):
        self.artifact_detector = ArtifactDetector()
        self.fixer = ClaudeFixup()

    def filter(self, transcript: dict) -> dict:
        """
        Run complete filtering pipeline.

        Returns:
            {
                "passed": bool,
                "transcript": dict | None,  # Fixed transcript or None if rejected
                "reason": str,  # Rejection reason if failed
                "fixup_log": list[dict],  # Record of fixes applied
            }
        """
        fixup_log = []
        current = transcript.copy()

        # Stage 1: Artifact detection
        artifacts = self.artifact_detector.detect(current)

        if artifacts:
            # Find first artifact that requires truncation
            truncate_at = None
            for artifact in artifacts:
                if artifact.truncate_before:
                    truncate_at = artifact.exchange_number
                    break

            if truncate_at is not None:
                current = truncate_transcript(current, truncate_at, self.MIN_EXCHANGES)

                if current is None:
                    return {
                        "passed": False,
                        "transcript": None,
                        "reason": "too_short_after_truncation",
                        "fixup_log": fixup_log,
                        "artifacts": [a.__dict__ for a in artifacts]
                    }

                fixup_log.append({
                    "stage": "artifact_truncation",
                    "action": f"Truncated at exchange {truncate_at}",
                    "reason": artifacts[0].description
                })

        # Stage 2: Rubric assessment
        assessment = assess_transcript(current)

        # Safety gate failures are auto-reject (never fix)
        if hasattr(assessment, 'safety_gate_failed') and assessment.safety_gate_failed:
            return {
                "passed": False,
                "transcript": None,
                "reason": "safety_gate_failed",
                "fixup_log": fixup_log,
                "assessment": assessment.__dict__ if hasattr(assessment, '__dict__') else str(assessment)
            }

        # If passed rubric, we're done!
        if assessment.passed:
            return {
                "passed": True,
                "transcript": current,
                "reason": "passed_all_filters",
                "fixup_log": fixup_log
            }

        # Stage 3: Claude fixup for rubric failures
        fixed_transcript = self._attempt_fixup(current, assessment, fixup_log)

        if fixed_transcript is None:
            return {
                "passed": False,
                "transcript": None,
                "reason": "rubric_failed_unfixable",
                "fixup_log": fixup_log,
                "fixup_attempted": True
            }

        # Stage 4: Re-assess after fixup
        final_assessment = assess_transcript(fixed_transcript)

        if final_assessment.passed:
            return {
                "passed": True,
                "transcript": fixed_transcript,
                "reason": "passed_after_fixup",
                "fixup_log": fixup_log
            }
        else:
            return {
                "passed": False,
                "transcript": None,
                "reason": "rubric_failed_after_fixup",
                "fixup_log": fixup_log,
                "fixup_attempted": True
            }

    def _attempt_fixup(
        self,
        transcript: dict,
        assessment,
        fixup_log: list[dict]
    ) -> dict | None:
        """
        Attempt to fix exchanges that failed rubric.

        Returns fixed transcript or None if unfixable.
        """
        # Identify which exchanges failed
        # This depends on assessment structure - adapt to your assessor.py

        # For now, we'll just try to fix exchanges that had issues
        # (This is a simplified version - real implementation needs to parse assessment.failed_checks)

        fixed = transcript.copy()
        fixed["conversations"] = fixed["conversations"].copy()

        # Try to fix each exchange (naive approach - could be smarter)
        for i in range(len(fixed["conversations"])):
            # Determine failed criteria for this exchange
            # (Requires assessor to provide per-exchange failures)
            failed_criteria = self._get_failed_criteria_for_exchange(assessment, i)

            if not failed_criteria:
                continue  # No issues with this exchange

            # Get next user message for entailment constraint
            if i + 1 < len(fixed["conversations"]):
                next_user_msg = fixed["conversations"][i + 1]["user"]
            else:
                next_user_msg = None

            # Attempt fix
            fixed_response = self.fixer.fix_exchange(
                history=fixed["conversations"][:i],
                exchange_number=i,
                problematic_response=fixed["conversations"][i]["assistant"],
                failed_criteria=failed_criteria,
                next_user_message=next_user_msg
            )

            if fixed_response is None:
                # Unfixable - truncate here instead
                truncated = truncate_transcript(fixed, i, self.MIN_EXCHANGES)

                if truncated is None:
                    return None  # Too short after truncation

                fixup_log.append({
                    "stage": "fixup_truncation",
                    "exchange": i,
                    "reason": "Fix would break continuity",
                    "action": f"Truncated at exchange {i}"
                })

                return truncated

            # Apply fix
            fixed["conversations"][i]["assistant"] = fixed_response
            fixed["conversations"][i]["fixed"] = True
            fixed["conversations"][i]["original_issues"] = failed_criteria

            fixup_log.append({
                "stage": "claude_fixup",
                "exchange": i,
                "criteria_fixed": failed_criteria,
                "action": "Rewrote assistant response"
            })

        return fixed

    def _get_failed_criteria_for_exchange(self, assessment, exchange_num: int) -> list[str]:
        """
        Extract failed criteria for specific exchange.

        TODO: This depends on assessor.py structure.
        For now, return generic failures.
        """
        # Placeholder - adapt to your assessor.py
        if hasattr(assessment, 'failed_checks') and assessment.failed_checks:
            return assessment.failed_checks[:3]  # Take first few
        return []
```

### Step 4: Run tests

```bash
uv run pytest tests/integration/test_filter_pipeline.py -v
```

Expected: Some tests may pass, some may fail depending on assessor.py integration

### Step 5: Commit

```bash
git add src/filters/pipeline.py tests/integration/test_filter_pipeline.py
git commit -m "feat: add multi-stage filter pipeline

- Stage 1: Artifact detection + truncation
- Stage 2: Rubric assessment
- Stage 3: Claude fixup (with entailment constraint)
- Stage 4: Re-assessment after fixup
- Returns filtered transcript or rejection reason + fixup log"
```

---

## Task 5: Per-Transcript Random Slicing

**Files:**
- Create: `src/slicing/random_slicer.py`
- Create: `tests/unit/test_random_slicer.py`

### Step 1: Write failing test

**File:** `tests/unit/test_random_slicer.py`

```python
import pytest
from src.slicing.random_slicer import get_varied_slice_points


def test_different_transcripts_get_different_slices():
    """Each transcript should get unique random slice points."""
    points_a = get_varied_slice_points(100, "transcript_001")
    points_b = get_varied_slice_points(100, "transcript_002")

    # Should be different (unless hash collision, which is ~0%)
    assert points_a != points_b


def test_same_transcript_id_gives_reproducible_slices():
    """Same transcript ID should always give same slices (reproducible)."""
    points_1 = get_varied_slice_points(100, "transcript_001")
    points_2 = get_varied_slice_points(100, "transcript_001")

    assert points_1 == points_2


def test_slice_density_increases_toward_end():
    """Slices should be sparse early, dense late."""
    points = get_varied_slice_points(100, "transcript_test")

    # Early quarter (0-25): should have ~3-5 slices (sparse)
    early = [p for p in points if p < 25]
    assert 2 <= len(early) <= 6

    # Late portion (60-100): should have ~15-20 slices (dense)
    late = [p for p in points if p >= 60]
    assert 10 <= len(late) <= 25


def test_always_includes_final_turn():
    """Final turn should always be included."""
    for total_turns in [20, 50, 75, 100]:
        points = get_varied_slice_points(total_turns, f"test_{total_turns}")
        assert total_turns in points
```

### Step 2: Run test to verify it fails

```bash
uv run pytest tests/unit/test_random_slicer.py::test_different_transcripts_get_different_slices -v
```

Expected: `FAIL - ImportError: cannot import name 'get_varied_slice_points'`

### Step 3: Implement random slicer

**File:** `src/slicing/random_slicer.py`

```python
"""
Per-transcript random slicing.

CRITICAL: Each transcript gets unique random slice points (seeded by transcript ID)
to prevent model from learning fixed slice patterns.
"""
import random


def get_varied_slice_points(total_turns: int, transcript_id: str) -> list[int]:
    """
    Generate unique random slice points for a transcript.

    Density increases toward end:
    - Early (0-25%): sparse (every 5-7 turns)
    - Middle (25-60%): moderate (every 3-5 turns)
    - Late (60-100%): dense (every 2-3 turns)

    Args:
        total_turns: Total number of turns in transcript
        transcript_id: Unique ID for reproducible randomness

    Returns:
        Sorted list of slice points (turn numbers)
    """
    # Seed with transcript ID hash for reproducibility
    random.seed(hash(transcript_id))

    points = []

    # Define phase boundaries
    early_end = total_turns // 4  # 25%
    mid_end = int(total_turns * 0.6)  # 60%

    # Early phase: sparse (5-7 turn intervals)
    current = random.randint(3, 5)
    while current < early_end:
        points.append(current)
        current += random.randint(5, 7)

    # Middle phase: moderate (3-5 turn intervals)
    while current < mid_end:
        points.append(current)
        current += random.randint(3, 5)

    # Late phase: dense (2-3 turn intervals)
    while current < total_turns:
        points.append(current)
        current += random.randint(2, 3)

    # Always include final turn
    if total_turns not in points:
        points.append(total_turns)

    return sorted(points)
```

### Step 4: Run tests

```bash
uv run pytest tests/unit/test_random_slicer.py -v
```

Expected: All `PASSED`

### Step 5: Commit

```bash
git add src/slicing/random_slicer.py tests/unit/test_random_slicer.py
git commit -m "feat: add per-transcript random slicing

- Each transcript gets unique random slice points (seeded by ID)
- Prevents model from learning fixed slice patterns
- Density increases toward end (sparse early, dense late)
- Always includes final turn
- Reproducible (same ID = same slices)"
```

---

## Task 6: Token Validation

**Files:**
- Create: `src/validation/token_counter.py`
- Create: `tests/unit/test_token_counter.py`

### Step 1: Write failing test

**File:** `tests/unit/test_token_counter.py`

```python
import pytest
from src.validation.token_counter import estimate_tokens, validate_example_length


def test_estimates_tokens():
    """Estimate token count for a message list."""
    messages = [
        {"role": "system", "content": "You are a therapeutic coach."},
        {"role": "user", "content": "I'm feeling anxious."},
        {"role": "assistant", "content": "That's completely valid. Tell me more about what's causing the anxiety."}
    ]

    tokens = estimate_tokens(messages)

    # Rough estimate: ~50 tokens total
    assert 30 < tokens < 100


def test_validates_example_within_limit():
    """Example under 120K tokens should pass."""
    example = {
        "messages": [
            {"role": "system", "content": "Short system prompt."},
            {"role": "user", "content": "Hi."},
            {"role": "assistant", "content": "Hello."}
        ]
    }

    assert validate_example_length(example) is True


def test_rejects_example_over_limit():
    """Example over 120K tokens should fail."""
    # Create a very long message (simulate 120K+ tokens)
    long_content = "word " * 40000  # ~120K tokens (3 tokens per word average)

    example = {
        "messages": [
            {"role": "system", "content": "System."},
            {"role": "user", "content": long_content},
            {"role": "assistant", "content": "Response."}
        ]
    }

    assert validate_example_length(example) is False
```

### Step 2: Run test to verify it fails

```bash
uv run pytest tests/unit/test_token_counter.py::test_estimates_tokens -v
```

Expected: `FAIL - ImportError: cannot import name 'estimate_tokens'`

### Step 3: Implement token counter

**File:** `src/validation/token_counter.py`

```python
"""
Token counting and validation.

Ensures training examples fit within Gemma 3 12B's 128K context window.
"""


def estimate_tokens(messages: list[dict]) -> int:
    """
    Estimate token count for a list of messages.

    Uses rough heuristic: 1 token ≈ 0.75 words ≈ 4 characters
    (This is approximate - real tokenization varies by model)

    For more accuracy, could use tiktoken or transformers tokenizer,
    but this heuristic is fast and sufficient for validation.
    """
    total_chars = sum(len(m["content"]) for m in messages)

    # Add overhead for role tags, formatting
    overhead = len(messages) * 10  # ~10 tokens per message for <role> tags

    # Rough estimate: 4 chars per token
    estimated_tokens = (total_chars // 4) + overhead

    return estimated_tokens


def validate_example_length(
    example: dict,
    max_tokens: int = 120000
) -> bool:
    """
    Validate that example fits within token limit.

    Args:
        example: Training example with "messages" key
        max_tokens: Maximum tokens allowed (default 120K, leaves buffer for 128K limit)

    Returns:
        True if example is within limit
    """
    tokens = estimate_tokens(example["messages"])

    return tokens <= max_tokens
```

### Step 4: Run tests

```bash
uv run pytest tests/unit/test_token_counter.py -v
```

Expected: All `PASSED`

### Step 5: Commit

```bash
git add src/validation/token_counter.py tests/unit/test_token_counter.py
git commit -m "feat: add token counting and validation

- Estimates tokens using char-based heuristic (~4 chars per token)
- Validates examples are under 120K token limit (buffer for 128K context window)
- Fast estimation sufficient for filtering"
```

---

## Task 7: Full-Conversation Evaluation

**Files:**
- Create: `src/evaluation/full_conversation_eval.py`
- Create: `tests/integration/test_full_conversation_eval.py`

### Step 1: Write failing test

**File:** `tests/integration/test_full_conversation_eval.py`

```python
import pytest
from src.evaluation.full_conversation_eval import evaluate_model_full_conversations
from unittest.mock import Mock


def test_evaluates_both_models_with_same_personas():
    """Evaluation should use same personas for both models (paired comparison)."""
    # Mock models
    base_model = Mock()
    base_model.name = "gemma-base"
    base_model.generate = Mock(return_value="Base model response.")

    ft_model = Mock()
    ft_model.name = "gemma-finetuned"
    ft_model.generate = Mock(return_value="Fine-tuned model response.")

    # Mock personas
    personas = [
        {"id": "persona_001", "name": "Test User 1"},
        {"id": "persona_002", "name": "Test User 2"}
    ]

    # Run evaluation
    base_scores = evaluate_model_full_conversations(
        model=base_model,
        personas=personas,
        trials_per_persona=2,  # 2 conversations each
        target_turns=10  # Short for testing
    )

    ft_scores = evaluate_model_full_conversations(
        model=ft_model,
        personas=personas,
        trials_per_persona=2,
        target_turns=10
    )

    # Should have 2 personas × 2 trials = 4 scores each
    assert len(base_scores) == 4
    assert len(ft_scores) == 4

    # Each score should be between 0 and 1
    assert all(0 <= s <= 1 for s in base_scores)
    assert all(0 <= s <= 1 for s in ft_scores)
```

### Step 2: Run test to verify it fails

```bash
uv run pytest tests/integration/test_full_conversation_eval.py::test_evaluates_both_models_with_same_personas -v
```

Expected: `FAIL - ImportError: cannot import name 'evaluate_model_full_conversations'`

### Step 3: Implement full-conversation eval

**File:** `src/evaluation/full_conversation_eval.py`

```python
"""
Full-conversation evaluation.

Generates complete conversations with both base and fine-tuned models,
assesses entire transcripts, compares scores.
"""
from assessor import assess_transcript
from typing import Any


def evaluate_model_full_conversations(
    model: Any,
    personas: list[dict],
    trials_per_persona: int = 3,
    target_turns: int = 50
) -> list[float]:
    """
    Generate full conversations and assess them.

    Args:
        model: Model to evaluate (must have .generate() method)
        personas: List of user personas
        trials_per_persona: Number of conversations per persona
        target_turns: Target conversation length

    Returns:
        List of assessment scores (one per conversation)
    """
    scores = []

    for persona in personas:
        for trial in range(trials_per_persona):
            # Generate full conversation
            transcript = generate_full_conversation(
                model=model,
                persona=persona,
                target_turns=target_turns,
                seed=trial
            )

            # Assess using existing rubric
            assessment = assess_transcript(transcript)

            # Extract score
            if hasattr(assessment, 'score'):
                score = assessment.score
            else:
                # Fallback if assessment structure differs
                score = float(assessment)

            scores.append(score)

            # Save for later review
            save_eval_transcript(
                transcript,
                assessment,
                model_name=model.name,
                persona_id=persona["id"],
                trial=trial
            )

    return scores


def generate_full_conversation(
    model: Any,
    persona: dict,
    target_turns: int,
    seed: int
) -> dict:
    """
    Generate a full conversation between model and simulated user.

    Args:
        model: Model to generate assistant responses
        persona: User persona for simulation
        target_turns: Target number of exchanges
        seed: Seed for user simulator (reproducibility)

    Returns:
        Complete transcript
    """
    # TODO: Implement user simulator integration
    # For now, this is a placeholder

    conversation = []

    for turn in range(target_turns):
        # User simulator generates message
        # (This requires user_simulator.py - not yet implemented)
        user_message = f"User message {turn + 1}"  # Placeholder

        # Model generates response
        # Build context from conversation history
        context = build_context(conversation)
        context.append({"role": "user", "content": user_message})

        assistant_response = model.generate(context)

        # Add exchange
        conversation.append({
            "exchange_number": turn + 1,
            "user": user_message,
            "assistant": assistant_response
        })

    return {
        "id": f"eval_{model.name}_{persona['id']}_trial{seed}",
        "persona": persona,
        "conversations": conversation,
        "model": model.name,
        "eval_metadata": {
            "seed": seed,
            "target_turns": target_turns
        }
    }


def build_context(conversation: list[dict]) -> list[dict]:
    """Build message list from conversation history."""
    messages = [
        {"role": "system", "content": "You are a therapeutic coach..."}  # TODO: Load from config
    ]

    for exchange in conversation:
        messages.append({"role": "user", "content": exchange["user"]})
        messages.append({"role": "assistant", "content": exchange["assistant"]})

    return messages


def save_eval_transcript(
    transcript: dict,
    assessment: Any,
    model_name: str,
    persona_id: str,
    trial: int
):
    """Save evaluation transcript for later review."""
    import json
    import os

    os.makedirs("data/evaluation/transcripts", exist_ok=True)

    output = {
        "transcript": transcript,
        "assessment": {
            "score": assessment.score if hasattr(assessment, 'score') else float(assessment),
            "passed": assessment.passed if hasattr(assessment, 'passed') else (assessment.score >= 0.80)
        },
        "metadata": {
            "model": model_name,
            "persona": persona_id,
            "trial": trial
        }
    }

    filename = f"data/evaluation/transcripts/{model_name}_{persona_id}_trial{trial}.json"

    with open(filename, "w") as f:
        json.dump(output, f, indent=2)
```

### Step 4: Run tests

```bash
uv run pytest tests/integration/test_full_conversation_eval.py -v
```

Expected: May pass with mocked models

### Step 5: Commit

```bash
git add src/evaluation/full_conversation_eval.py tests/integration/test_full_conversation_eval.py
git commit -m "feat: add full-conversation evaluation

- Generates complete conversations (not just next-turn prediction)
- Assesses entire transcripts with existing rubric
- Saves evaluation transcripts for manual review
- Returns list of scores for statistical comparison"
```

---

## Task 8: Statistical Comparison

**Files:**
- Create: `src/evaluation/statistical_comparison.py`
- Create: `tests/unit/test_statistical_comparison.py`

### Step 1: Write failing test

**File:** `tests/unit/test_statistical_comparison.py`

```python
import pytest
import numpy as np
from src.evaluation.statistical_comparison import compare_models


def test_detects_significant_improvement():
    """Detect statistically significant improvement."""
    # Base model: scores around 0.70
    base_scores = [0.70 + np.random.normal(0, 0.05) for _ in range(30)]

    # Fine-tuned: scores around 0.85 (clear improvement)
    ft_scores = [0.85 + np.random.normal(0, 0.05) for _ in range(30)]

    result = compare_models(base_scores, ft_scores)

    assert result["improvement"] > 0.10
    assert result["p_value"] < 0.05
    assert result["significant"] is True


def test_detects_no_significant_difference():
    """Detect when there's no significant difference."""
    # Both models similar
    base_scores = [0.75 + np.random.normal(0, 0.10) for _ in range(30)]
    ft_scores = [0.76 + np.random.normal(0, 0.10) for _ in range(30)]

    result = compare_models(base_scores, ft_scores)

    assert result["p_value"] > 0.05
    assert result["significant"] is False
```

### Step 2: Run test to verify it fails

```bash
uv run pytest tests/unit/test_statistical_comparison.py::test_detects_significant_improvement -v
```

Expected: `FAIL - ImportError: cannot import name 'compare_models'`

### Step 3: Implement statistical comparison

**File:** `src/evaluation/statistical_comparison.py`

```python
"""Statistical comparison of model performance."""
import numpy as np
from scipy.stats import ttest_rel


def compare_models(
    base_scores: list[float],
    finetuned_scores: list[float],
    alpha: float = 0.05
) -> dict:
    """
    Compare base vs fine-tuned model using paired t-test.

    Args:
        base_scores: Scores from base model
        finetuned_scores: Scores from fine-tuned model (same personas/trials)
        alpha: Significance level (default 0.05)

    Returns:
        Dictionary with comparison results
    """
    base_mean = np.mean(base_scores)
    base_std = np.std(base_scores)
    ft_mean = np.mean(finetuned_scores)
    ft_std = np.std(finetuned_scores)

    improvement = ft_mean - base_mean
    improvement_pct = (improvement / base_mean) * 100

    # Paired t-test (same personas for both models)
    t_stat, p_value = ttest_rel(base_scores, finetuned_scores)

    # Pass rate comparison (≥0.80 threshold)
    base_pass_rate = sum(1 for s in base_scores if s >= 0.80) / len(base_scores)
    ft_pass_rate = sum(1 for s in finetuned_scores if s >= 0.80) / len(finetuned_scores)

    return {
        "base_model": {
            "mean": base_mean,
            "std": base_std,
            "pass_rate": base_pass_rate,
            "n": len(base_scores)
        },
        "finetuned_model": {
            "mean": ft_mean,
            "std": ft_std,
            "pass_rate": ft_pass_rate,
            "n": len(finetuned_scores)
        },
        "improvement": improvement,
        "improvement_pct": improvement_pct,
        "t_statistic": t_stat,
        "p_value": p_value,
        "significant": p_value < alpha,
        "alpha": alpha
    }


def print_comparison_report(result: dict):
    """Print human-readable comparison report."""
    print(f"""
Evaluation Results
{'=' * 80}

Base Model:
  Mean Score:  {result['base_model']['mean']:.3f} ± {result['base_model']['std']:.3f}
  Pass Rate:   {result['base_model']['pass_rate']:.1%}
  N:           {result['base_model']['n']}

Fine-tuned Model:
  Mean Score:  {result['finetuned_model']['mean']:.3f} ± {result['finetuned_model']['std']:.3f}
  Pass Rate:   {result['finetuned_model']['pass_rate']:.1%}
  N:           {result['finetuned_model']['n']}

Improvement:
  Absolute:    {result['improvement']:+.3f}
  Percentage:  {result['improvement_pct']:+.1f}%
  Pass Rate:   {(result['finetuned_model']['pass_rate'] - result['base_model']['pass_rate']):.1%}

Statistical Test (Paired t-test):
  t-statistic: {result['t_statistic']:.3f}
  p-value:     {result['p_value']:.4f}
  Significant: {'✓ YES' if result['significant'] else '✗ NO'} (α={result['alpha']})

{'=' * 80}
""")
```

### Step 4: Run tests

```bash
uv run pytest tests/unit/test_statistical_comparison.py -v
```

Expected: All `PASSED`

### Step 5: Commit

```bash
git add src/evaluation/statistical_comparison.py tests/unit/test_statistical_comparison.py
git commit -m "feat: add statistical model comparison

- Paired t-test for base vs fine-tuned scores
- Computes mean, std, pass rate for both models
- Reports improvement (absolute and percentage)
- Determines statistical significance (p < 0.05)
- Pretty-prints comparison report"
```

---

## Summary of Implementation

### Completed Components

1. ✅ **Artifact Detection** - Detects truncation, meta-commentary, character breaks
2. ✅ **Truncation** - Truncates at artifacts, rejects if < 10 exchanges remain
3. ✅ **Claude Fixup** - Fixes rubric failures with entailment constraint
4. ✅ **Filter Pipeline** - Orchestrates artifact → fixup → assessment
5. ✅ **Random Slicing** - Per-transcript unique slices (prevents pattern learning)
6. ✅ **Token Validation** - Ensures examples fit in 128K context window
7. ✅ **Full-Conversation Eval** - Generates & assesses complete conversations
8. ✅ **Statistical Comparison** - Paired t-test with significance testing

### Still TODO (Out of Scope for This Plan)

These require separate implementation:
- User simulator (generates realistic user messages)
- Transcript generation orchestration (calls user sim + model in loop)
- Training data transformation to HuggingFace format
- QLora training setup (TRL/PEFT configuration)
- GGUF export for local inference

### File Structure

```
src/
├── filters/
│   ├── artifact_detection.py
│   ├── truncation.py
│   └── pipeline.py
├── fixup/
│   └── claude_fixup.py
├── slicing/
│   └── random_slicer.py
├── validation/
│   └── token_counter.py
└── evaluation/
    ├── full_conversation_eval.py
    └── statistical_comparison.py

tests/
├── unit/
│   ├── test_artifact_detection.py
│   ├── test_truncation.py
│   ├── test_random_slicer.py
│   ├── test_token_counter.py
│   └── test_statistical_comparison.py
└── integration/
    ├── test_filter_pipeline.py
    ├── test_claude_fixup.py
    └── test_full_conversation_eval.py
```

### Testing Strategy

**Unit Tests:**
- Fast, isolated, no external dependencies
- Test individual functions and classes
- Run frequently during development

**Integration Tests:**
- Test component interactions
- May call Claude API (slower, potentially flaky)
- Run before committing

**Run all tests:**
```bash
uv run pytest tests/ -v
```

**Run only unit tests:**
```bash
uv run pytest tests/unit/ -v
```

---

## Critical Gotchas

### 🚨 GOTCHA 1: Claude Fixup is Expensive

Each fixup calls Claude API. With 155 transcripts × average 5 failing exchanges = ~775 API calls for fixup.

**Mitigation:**
- Only fix transcripts that are "close" to passing (e.g., failed by <2 criteria)
- Reject transcripts with many failures rather than attempting fixup
- Monitor fixup success rate - if low, improve generation prompts instead

### 🚨 GOTCHA 2: Entailment Constraint May Be Too Strict

If Claude often returns "UNFIXABLE", the entailment constraint may be too strict.

**Mitigation:**
- Track "unfixable" rate
- If > 30%, consider relaxing constraint
- Or: improve generation prompts so fewer fixes needed

### 🚨 GOTCHA 3: Per-Transcript Slicing Seed

Using `hash(transcript_id)` as seed means different Python versions might give different slices (hash is not guaranteed stable across versions).

**Mitigation:**
- Use `hashlib` for stable hashing if reproducibility across environments matters
- Or: accept that slices may differ across Python versions (not critical)

### 🚨 GOTCHA 4: Token Estimation is Approximate

Character-based estimation may be off by 10-20% vs actual tokenization.

**Mitigation:**
- Use conservative limit (120K instead of 128K)
- For production, consider using actual tokenizer (slower but accurate)

### 🚨 GOTCHA 5: Evaluation Requires User Simulator

Full-conversation eval needs a user simulator that hasn't been implemented yet.

**Mitigation:**
- Implement user simulator as separate task
- Or: use simpler evaluation (model generates responses to pre-written user messages)

---

## Next Steps

After completing this plan:

1. **Implement User Simulator** - Required for transcript generation and evaluation
2. **Build Generation Orchestration** - Loop that calls user sim + model to create transcripts
3. **Create Training Pipeline** - Transform to HuggingFace format, set up QLora training
4. **Run Pilot** - Generate 10 transcripts, validate pipeline, iterate
5. **Scale Generation** - Generate remaining 145 transcripts
6. **Train Model** - QLora fine-tuning on filtered data
7. **Evaluate** - Full-conversation eval, statistical comparison
8. **Export GGUF** - For local inference on Mac/Ollama

---

## DRY, YAGNI, TDD Principles

**DRY (Don't Repeat Yourself):**
- Shared logic in base classes (ArtifactDetector, ClaudeFixup)
- Reusable functions (estimate_tokens, truncate_transcript)
- Common test fixtures (avoid duplicating test data)

**YAGNI (You Aren't Gonna Need It):**
- No premature optimization (simple char-based token estimation, not complex tokenizer)
- No fancy caching or parallelization until proven necessary
- No configuration files until we have >2 values to configure

**TDD (Test-Driven Development):**
- Write failing test first
- Implement minimal code to pass
- Refactor if needed
- Commit frequently

**Commit frequency:** After each passing test (every 2-5 minutes ideally)

---

_End of Implementation Plan_
