# MLX LoRA Fine-tuning Configuration
# For therapeutic coaching model validation run

model: ~/models/gemma-3-12b-mlx-bf16
data: data/processed/mlx_training
adapter_path: adapters/therapeutic-gemma-12b

# Training parameters
train: true
iters: 1400                   # ~3 epochs over 476 examples
batch_size: 1                 # Keep low for long sequences
learning_rate: 1e-5           # Conservative for fine-tuning
grad_accumulation_steps: 8    # Effective batch size of 8

# LoRA parameters
fine_tune_type: lora
num_layers: -1                # All layers

# Reporting (no validation)
val_batches: 0                # No validation set
steps_per_report: 50
save_every: 500

# Optimization
mask_prompt: true             # Only train on assistant responses
max_seq_length: 8192          # Reasonable context for our data

# Reproducibility
seed: 42
