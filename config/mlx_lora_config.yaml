# MLX LoRA Fine-tuning Configuration
# For therapeutic coaching model validation run

model: models/gemma-3-12b-mlx-8bit
data: data/processed/mlx_training
adapter_path: adapters/therapeutic-gemma-12b

# Training parameters
train: true
iters: 600                    # ~8 epochs over 203 examples (effective batch 8)
batch_size: 1                 # Keep low for long sequences
learning_rate: 1e-5           # Conservative for fine-tuning
grad_accumulation_steps: 8    # Effective batch size of 8

# LoRA parameters
fine_tune_type: lora
num_layers: -1                # All layers

# Reporting
val_batches: 1                # Minimal validation
steps_per_report: 50
save_every: 200

# Optimization
mask_prompt: true             # Only train on assistant responses
max_seq_length: 16384         # Matches our 16K token filter
grad_checkpoint: true         # Trade compute for memory

# Reproducibility
seed: 42
